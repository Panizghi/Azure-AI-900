# Module 1

# **Define cloud computing**

- Cloud computing offers the use of computing resources as a service over a network.
- The "cloud" is simply a large distributed computing infrastructure whose services are available to end users in much the same that electricity is available to customers of power companies.

# **What is Cloud Computing?**

### **Cloud Computing**

*(definition)* The delivery of computing as a service over a network, whereby distributed resources are provided to the end user as a utility.

![https://docs.microsoft.com/en-us/learn/cmu-cloud-admin/cmu-cloud-admin-overview/media/it_components.png](https://docs.microsoft.com/en-us/learn/cmu-cloud-admin/cmu-cloud-admin-overview/media/it_components.png)

Traditionally, an organization that needs to deploy a particular IT solution has to procure, set up, and maintain the infrastructure and the application. The organization hence "owns" the solution, which allows full control over the solution, However, **ownership** has some drawbacks:

1. Organizations must **pay up-front** to buy a particular solution
2. Organizations are solely responsible for managing their IT solutions. Result in addition to **up-front costs, organizations must budget for recurring costs.**
3. The IT solution typically has a fixed size; as the number of employees or customers grows, the organization must **purchase additional hardware and/or software to keep up with increasing demand.**
4. Typically, on-premises IT systems suffer from **low average *utilization***, which refers to the proportion of time (expressed usually as a percentage) that an IT system is being used to capacity. 

**Cloud computing is the transformation of owned IT products into services that can be leased from a *cloud service provider* (CSP) such as Amazon, Alibaba, Microsoft, or Google**.

The cloud model offers users and organizations several benefits, including:

- **Reduced up-front cost, as IT services can be obtained in a pay-as-you-go model**
- **The convenience of fast resource provisioning, which significantly reduces the time to market for IT solutions**
- **Rapid scalability of computing resources, as they can be scaled up and down on demand.**
- **Cloud providers' resources are shared by multiple users, thereby improving utilization and reducing carbon footprint.**

# **Before the cloud**

**Business computing**: Business computing frequently involves the use of management-information systems that drive logistics and operations, enterprise resource planning (ERP), customer relation management (CRM), office productivity, and business intelligence (BI). Such tools enabled more streamlined processes that led to improved productivity and reduced cost across a variety of enterprises.

**Scientific computing**: Scientific computing uses mathematical models and analysis techniques implemented on computers to attempt to solve scientific problems. A popular example is computer simulation of physical phenomena -- for example, using climate models to predict the weather. This field has disrupted the traditional theoretical and laboratory experimental methods by enabling scientists and engineers to reconstruct known events or to predict future situations by developing programs to simulate and study different systems under different circumstances. Such simulations typically require a very large number of calculations which are often run on expensive supercomputers or distributed computing platforms.

**Personal computing**: In personal computing, a user runs various applications on a general-purpose personal computer (PC). Examples of such applications include word processors and other office-productivity software, communication apps such as email clients, and entertainment software such as video games and media players. A PC user typically owns, installs, and maintains the software and hardware utilized to carry out such tasks.

# **Addressing Scale**

One of the longstanding challenges in IT is **scaling compute resources to meet demand** scale as they plan and budget for the deployment of their solutions.

Organizations typically plan their IT infrastructure in a process called *capacity planning*. During the capacity planning process, the growth in usage of various IT services is gauged and used as a benchmark for future expansion. Organizations have to plan in advance to procure, set up, and maintain newer and better servers, storage devices, and networking equipment. 

The most basic form of scaling is known as ***vertical scaling*, whereby old systems are replaced with newer, better-performing systems with faster CPUs and more memory and disk space.** In many cases, vertical scaling consists of **upgrading or replacing servers and adding capacity to storage arrays.** 

Scaling can also be done ***horizontally* by increasing or decreasing the number of resources dedicated to the system.** An example of this is in high-performance computing, where additional **servers and storage capacity can be added to existing clusters**, thereby increasing the number of calculations that can be performed per second. 

 **Large companies consolidated the computing needs of different departments into a single large data center** whereby they consolidated real estate, power, cooling, and networking in order to reduce cost. On the other hand, **small and medium-size companies could lease real estate, network, power, cooling, and physical security by placing their IT equipment in a shared data center**. This is typically referred to as a *co-location service* which was adopted by small to medium-sized companies who did not want to build their own data centers in-house. 

In scientific computing, parallel and distributed systems have been adopted in order to scale up the size of the problems and the precision of their numerical simulations. 

**One definition of *parallel processing* is the use of multiple homogenous computers that share state and function as a single large computer in order to run large scale or high precision calculations.** 

***Distributed computing* is the use of multiple autonomous computing systems connected by a network in order to partition a large problem into subtasks that execute concurrently and communicate via messages over the network**. 

The scientific community continued to innovate in these domains in order to address scale. In personal computing, scale has had an impact through increased demands brought on by richer content and memory-hungry applications. Users replace their PCs with newer, faster models or upgrade existing models to keep up with these demands.

# **Rise of Internet Services**

![https://docs.microsoft.com/en-us/learn/cmu-cloud-admin/cmu-cloud-admin-overview/media/traditional_vs_internet_scale.png](https://docs.microsoft.com/en-us/learn/cmu-cloud-admin/cmu-cloud-admin-overview/media/traditional_vs_internet_scale.png)

The Internet has created a global marketplace of more than 4 billion users. This rise in data and connections is valuable to businesses. Data creates value in several ways, including by enabling experimentation, segmenting populations, and supporting decision-making with automation[1](https://www.ibm.com/analytics/hadoop/big-data-analytics). 

The increasing number of connections enabled by the Internet has also driven its value. Researchers have hypothesized that the value of a network varies super-linearly as a function of the number of users. Thus, at Internet scale, gaining and retaining customers is a priority, and this is done by building reliable and responsive services, and making changes based on observed data patterns.

Some examples of **Internet-scale systems** include:

1. **Search engines** that crawl, store, index, and search large (up to petabyte-sized) data sets. For instance, Google started as a giant web index that crawled and analyzed web traffic once every few days and matched these indices to keywords.
2. **Social networks** like Facebook and LinkedIn that allow users to create personal and professional relationships and build communities based on similar interests. Facebook, for instance, now boasts more than 2 billion active users per month.
3. **Online retail services** like Amazon maintain an inventory of millions of products and serve a global user base. In 2017, Amazon's online retail operation achieved net sales of $178 billion, up 31% from the year before.
4. **Rich, streaming multimedia applications** allow people to watch and share videos and other forms of rich content. One such example, YouTube, serves up 5 billion videos per day and has 300 minutes of video uploaded to it every second.
5. **Real-time communications systems** for audio, video, and text chatting like Skype which clock more than 50 billion minutes of calls per month.
6. **Productivity and collaboration suites** that serve millions of documents to many concurrent users allowing real-time, persistent updates. For example, Microsoft 365 serves about 60 million active users each month.
7. **CRM applications** by providers like SalesForce are deployed at over a hundred thousand organizations. Large CRMs now provide intuitive dashboards to track status, analytics to find the customers that generate the most business and revenue forecasting to predict future growth.
8. **Data mining and business intelligence** applications that analyze the usage of other services (like those above) to find inefficiencies and opportunities for monetization.

Clearly, these systems are expected to deal with a high volume of concurrent users. This requires an infrastructure with the capacity to handle large amounts of network traffic, generate and securely store data, all without any noticeable delays. These services derive their value by providing a constant and reliable standard of quality. They also provide rich user interfaces for mobile devices and web browsers, making them easy to use but harder to build and maintain.

We summarize some of the requirements of Internet-scale systems here:

1. **Ubiquity** --- **Being accessible from anywhere at any time**, from a multitude of devices. For instance, a salesperson will expect their CRM service to provide timely updates on a mobile device to make visits to clients shorter, faster, and more effective. The service should function smoothly under a variety of network connections.
2. **High availability** --- **The service must be "always up."** Uptimes are measured in terms of number of nines. Three nines, or 99.9%, implies that a service will be unavailable for 9 hours a year. Five nines (about 6 minutes a year) is a typical threshold for a high-availability service. Even a few minutes of downtime in online retail applications can impact millions of dollars of sales.
3. **Low latency** --- **Fast and responsive access times**. Even slightly slower page-load times have been shown to significantly reduce the usage of the affected web page. For instance, increasing search latency from 100 milliseconds to 400 milliseconds decreases the number of searches per user from 0.8% to 0.6%, and the change persists even after latency is restored to original levels.
4. **Scalability** --- **The ability to deal with variable loads resulting from seasonality and viralit**y, which causes peaks and troughs in the traffic over long and short periods of time. On days such as "Black Friday" and "Cyber Monday", retailers such as Amazon and Walmart receive several times the network traffic than on average.
5. **Cost effectiveness** --- An Internet-scale service requires significantly more infrastructure than a traditional application as well as better management. One way to streamline costs is by making services easier to manage and reducing the number of administrators handling a service. Smaller services can afford to have a **low service-to-admin ratio** (for example, 2:1, meaning a single administrator must maintain two services). However, to maintain profitability, services like Microsoft Bing must have high service-to-admin ratio (for example, **2500:1, meaning a single administrator maintains 2500 services).**
6. **Interoperability** --- **Many of these services are often used together and hence must provide an easy interface for reuse and support standardized mechanisms for importing and exporting data.** For example, services such as Uber may integrate Google Maps into their products to provide simplified location and navigation information to users.

The first challenge to be tackled was the **large round-trip time** for early web services that were mostly located in the United States. The earliest mechanisms to deal with the problems of low latency (due to distant servers) and server failure simply relied on redundancy. One technique for achieving this was by **"mirroring" content, whereby copies of popular web pages would be stored at different locations around the world**. This minimized the amount of load on the central server, reduced the latency experienced by end users, and allowed traffic to be switched over to another server in case of failures.

- The downside was an increase in complexity to deal with inconsistencies if even one copy of the data were to be modified. Thus, this technique is more useful for static, read-heavy workloads, such as serving images, videos, or music. Due to the effectiveness of this technique, most Internet-scale services use **content delivery networks (CDNs)** to store distributed global caches of popular content. For example, Cable News Network (CNN) now maintains replicas of their videos on multiple "edge" servers at different locations worldwide, with personalized advertising per location.

Cost efficiencies were often gained by using **shared hosting services**. Here, shares of a single web server would be leased out to multiple tenants, amortizing the cost of server maintenance. Shared hosting services could be highly resource-efficient, as the resources could be over-provisioned under the assumption that not all services would be operating at peak capacity at the same time. (An over-provisioned physical server is one where the aggregate capacity of all the tenants is greater than the actual capacity of the server.) 

- The downside was that it was nearly impossible to isolate the tenants' services from those of their neighbors. Thus, a single overloaded or error-prone service could adversely impact all its neighbors. Another problem arose because tenants could often be malicious and try to leverage their co-location advantage to steal data or deny service to other users.

To counter this, **virtual private servers** were developed as variants of the shared hosting model. A tenant would be provided with a virtual machine (VM) on a shared server. These VMs were often statically allocated and linked to a single physical machine, which meant they were difficult to scale and often needed manual recovery from any failures. Though they could no longer be overprovisioned, they had better performance and security isolation between co-located services than simple resource sharing.

- Another problem of sharing public resources was that it required storing private data on third-party infrastructure.
    - Some of the Internet-scale services described above could not afford to lose control over data storage, since any disclosure of their customers private data would have disastrous consequences. Hence, these companies needed to build their own global infrastructure. Before the advent of the public cloud, such services could only be deployed by large corporations like Google and Amazon. Each of these companies would build large, homogeneous data centers across the globe using off-the-shelf components, where a data center could be thought of as a single, massive **warehouse-scale computer (WSC)**. A WSC provided an easy abstraction to globally distribute applications and data, while still maintaining ownership.

Due to the economies of scale, the utilization of a data center could be optimized to reduce costs. Even though this was still not as efficient as publicly sharing resources (the cloud), these warehouse-scale computers had many desirable properties that served as foundations for building Internet-scale services. The scale of computing applications progressed from serving a fixed user base to serving a dynamic global population. Standardized WSCs allowed large companies to serve such large audiences. An ideal infrastructure would combine the performance and reliability of a WSC with the sharing hosting model. This would enable even a small corporation to develop and launch a globally competitive application, without the high overhead of building large data centers.

Another approach to share resources was **grid computing, which enabled the sharing of autonomous computing systems across institutions and geographical locations**. Several academic and scientific institutions would collaborate and pool their resources in pursuit of a common goal. Each institution would then join a "virtual organization" by dedicating a specific set resources via well-defined sharing rules. 

- Resources would often be heterogeneous and loosely coupled, requiring complex programming constructs to stitch together. Grids were geared towards supporting non-commercial research and academic projects and relied on existing open source technologies.

**The cloud was a logical successor that combined many of the features of the solutions above**. For example, instead of universities contributing and sharing access to a pool of resources using a grid, the cloud allows them to lease computing infrastructure that is centrally administered by a cloud service provider. As the central provider maintained a large resource pool to satisfy all clients, the cloud made it easier to dynamically scale up and down demand in a short period of time. Instead of open standards like the grid, however, cloud computing relies on proprietary protocols and requires the user to place a certain level of trust in the CSP.

# **Evolution of Computing**

Since the 1960s, some of the earliest forms of computers that were used by organizations were mainframe computers. 

- Multiple users could share and connect to mainframes over basic serial connections using terminals.
- The mainframe was responsible for all the **logic, storage, and processing of data, and the terminals connected to them had little if any computational power.**

With the birth of personal computing, cheaper, smaller, more powerful processors and memory led to a swing in the opposite direction, in which users ran their own software and stored data locally. This situation, in turn, led to issues of ineffective data sharing and rules to maintain order within an organization's IT environment.

Gradually, through the development of high-speed network technologies, **local area networks (LANs) were born that allowed computers to connect and communicate with each other.** Vendors designed systems that could encapsulate the benefits of both personal computers and mainframes, resulting in client-server applications that became popular over LANs. Clients would typically run client software (and process some data) or a terminal (for legacy applications) that connected to a server. The server, in the client-server model, provided application, storage, and data logic.

Eventually, in the 1990s, the global information age emerged with the Internet rapidly being adopted. Network bandwidth improved by many orders of magnitude, from ordinary dial-up access to dedicated fiber connectivity today. In addition, cheaper and more powerful hardware emerged. Furthermore, the evolution of the World Wide Web and dynamic websites necessitated multitier architectures.

Multitier architectures enabled the modularization of software by separating the application's presentation, logic, and storage layers as individual entities. With this modularization and decoupling, it was not long before individual software entities were running on distinct physical servers (typically due to differences in hardware and software requirements). This led to an increase of individual servers in organizations; however, it also led to poor average utilization of server hardware. In 2009, the International Data Corporation (IDC) estimated that the average x86 server had a utilization rate of approximately 5 to 10%.

Virtual machine technology matured well enough in the 2000s to become available as commercial software. 

Virtualization enables an entire server to be encapsulated as an image, which can be run seamlessly on hardware and enable multiple virtual servers to run simultaneously on a single physical server and share hardware resources. **Virtualization thus enables servers to be consolidated, which accordingly improves system utilization.**

Simultaneously, grid computing gained traction in the scientific community in an effort to solve large-scale problems in a distributed fashion. With **grid computing, computer resources from multiple administrative domains work in unison for a common goal**. Grid computing brought forth many resource-management tools (for example, schedulers and load balancers) to manage large-scale computing resources.

As the various computing technologies evolved, so did the economics of computing. Even during the early days of mainframe-based computing, companies such as IBM offered to host and run computers and software for various organizations such as banks and airlines. In the Internet Age, third-party web hosting also became popular. With virtualization, however, providers have unparalleled flexibility in accommodating multiple clients on a single server, sharing hardware and resources between them.

The development of these technologies, coupled with the economic model of utility computing, is what eventually evolved into cloud computing.

# **Enabling Technologies**

**Cloud computing has various enabling technologies, which include networking, virtualization and resource management, utility computing, programming models, parallel and distributed computing, and storage technologies.**

![https://docs.microsoft.com/en-us/learn/cmu-cloud-admin/cmu-cloud-admin-overview/media/enabling_technologies.png](https://docs.microsoft.com/en-us/learn/cmu-cloud-admin/cmu-cloud-admin-overview/media/enabling_technologies.png)

V**irtualization allows managing the complexity of the cloud via abstracting and sharing its resources across users through multiple virtual machines**. Each virtual machine can execute its own operating system and associated application programs.

Technologies such as large-scale storage systems, distributed file systems, and novel database architectures are crucial for managing and storing data in the cloud. Utility computing offers numerous charging structures for the leasing of compute resources. Examples include pay-per-resource-hour, pay-per-guaranteed-throughput, and pay-per-data stored per month, etc.

Parallel and distributed computing allow distributed entities located at networked computers to communicate and coordinate their actions in order to solve certain problems, represented as parallel programs. **Writing parallel programs for distributed clusters is inherently difficult. To achieve high programming efficiency and flexibility in the cloud, a programming model is required.**

Programming models for clouds give users the **flexibility to express parallel programs as sequential computation units** (for example, functions as in MapReduce and vertices as in GraphLab). Such programming models' runtime systems typically parallelize, distribute, and schedule computational units, manage inter-unit communication, and tolerate failures.

# **Cloud Building Blocks**

Figure 1.5 provides a useful abstraction of **cloud computing by dividing it into four layers: application software, development platforms, resource sharing, and infrastructure**. 

The infrastructure layer → includes the physical resources in a data center. 

The resource-sharing layer → uses a combination of hardware and software to facilitate the sharing of the physical resources while offering a certain level of isolation. 

The development-platform layer → provides functional support for cloud applications. 

application-software layer → hosts the applications themselves.

![https://docs.microsoft.com/en-us/learn/cmu-cloud-admin/cmu-cloud-admin-overview/media/cloud_computing_stack.png](https://docs.microsoft.com/en-us/learn/cmu-cloud-admin/cmu-cloud-admin-overview/media/cloud_computing_stack.png)

*Figure 1.5: Cloud computing building blocks.*

The four layers can be further quantified this way:

**Application software**: The top layer in the stack is the application software, which normally is the system component that the end user utilizes.

**Development platforms**: The next layer, development platforms, allows application developers to write application software utilizing the cloud provider's application programming interface (API). Each cloud platform provides callable APIs that developers can use to access cloud storage, authenticate users, and perform other necessary tasks.

**Resource sharing**: Resource-sharing mechanisms, the third layer, embody some key cloud ideas:

- Provide software, computation, network, and storage services.
- Allow a shared environment whereby multiple hardware images (for example, virtual machines) and system images (e.g., operating systems) can run side by side on a single server along with security, resource, and failure isolations.
- Consolidate physical servers into virtual servers that run on fewer physical servers.
- Deliver agility and elasticity to rapidly respond to users' resource and service demands.

These ideas usually are delivered through ***virtualization***, which relies on a **combination of software and firmware to create virtual machines that operate as if they were independent physical machines, even in cases where several virtual machines are hosted on a single physical server.**

**Infrastructure**: Physical resources comprise the bottom layer and, in cloud computing, are primarily deployed on the cloud provider's side. The broad resource classes include the following:

- Compute resources, typically servers, which are computers designed for enterprise computing (as opposed to user workstations). They usually are rack-mounted to utilize space efficiently.
- Storage resources maintain the cloud's data, and use of these resources is usually charged in terms of volume -- for example, charging per gigabyte or terabyte of storage consumed.
- Network resources enable communication between servers as well as servers and clients.
- Software that manages the compute, network and storage infrastructure.

# **Cloud Service Models**

Services offered by cloud providers fall into three categories known as *service models*:

- **IaaS, which is short for *Infrastructure-as-a-Service***
- **PaaS, which is short for *Platform-as-a-Service***
- **SaaS, which is short for *Software-as-a-Service***

![https://docs.microsoft.com/en-us/learn/cmu-cloud-admin/cmu-cloud-admin-overview/media/cloud-service-models.png](https://docs.microsoft.com/en-us/learn/cmu-cloud-admin/cmu-cloud-admin-overview/media/cloud-service-models.png)

*Figure 1.6: Cloud service models.*

In a traditional (non-cloud) computing environment, you own the hardware and the software and are responsible for deploying and maintaining them. 

**IaaS**, by contrast, the cloud service provider provides virtualized hardware in the form of virtual machines. You are responsible for keeping the operating system patched and installing and maintaining any software that runs in the virtual machine. But you are no longer responsible for maintaining hardware.

**PaaS** goes a step further and provides a platform for developing and running applications that you install and maintain. Significantly, the cloud provider takes responsibility for maintaining the platform itself. A great example of a **PaaS cloud service is Azure App Service**, which provides a complete platform for hosting web apps and allows you to focus on writing apps rather than installing and maintaining hardware and software.

**SaaS** provides a fully managed platform for accomplishing specific tasks. **Web mail apps are examples of SaaS; you connect with a browser and perform e-mail tasks without installing anything on your PC. You typically neither know nor care what type of server, real or virtual, is being used or what's installed on it**. Tasks such as updating and patching the software are performed by the cloud provider -- hence the term *fully managed*.

# **The Infrastructure-as-a-Service Model**

### **Infrastructure-as-a-service**

*(definition)* Infrastructure-as-a-service (IaaS) is a cloud-computing model in which cloud providers make computing resources available to clients, usually in the form of virtual machines

In the **IaaS model, providers rent out compute resources in the form of virtual machines** (also known as "instances") and let you choose from a wide variety of configurations -- for example, CPU, memory, disk, and network bandwidth. Once a VM is provisioned, an IaaS user can remotely connect to it and install platforms and applications. This model offers the most amount of flexibility to the IaaS users in terms of software development and deployment. Rather than purchasing servers, software, data center space, or network equipment, users rent these resources as a fully outsourced service on demand.

IaaS has the following characteristics:

- **Computing resources are provided to users as a service.**
- **IaaS providers provide tools that enable IaaS users to scale the resources that they deploy.**
- **IaaS providers usually have different resource offerings at different costs and follow a utility pricing model (typically calculated by the hour).**
- **The same physical resources are shared among multiple users.**

## **Examples of IaaS**

Amazon Web Services, Microsoft Azure, Google, and Rackspace are examples of cloud platforms that offer IaaS products. AWS's Elastic Compute Cloud (EC2) was one of the first commercially successful IaaS products. AWS EC2 rents out instances from various data centers scattered around the world. Users can choose from various instance types, from low-memory, single-CPU virtual machines that cost a few US dollars per month, all the way up to multicore, high-performance, GPU-accelerated instances that cost several hundred dollars per month. Azure's Virtual Machine service is similar and supports Linux as well as Windows. As of September 2018, roughly half of the virtual machines extant in Azure were Linux virtual machines[1](https://www.zdnet.com/article/linux-now-dominates-azure/).

## **Pricing Models**

**IaaS usually is priced on an hourly basis**. 

Cloud providers can also choose to bill on a prorated or non-prorated basis. **On a prorated basis, each partial hour is billed partially, while on a non-prorated basis, each partial hour is billed as a full hour.** This difference becomes significant when IaaS users need a large number of instances for a short period of time for burst processing. Today, popular cloud providers bill for most virtual machines on a per-second basis.

## **Use Cases for IaaS**

IaaS makes sense in a number of situations:

- For **new organizations that do not have the capital to invest** in infrastructure on site.
- When **organizations need to grow their IT resources rapidly**, such as Internet startup companies.
- For **temporary projects or temporary infrastructural needs** (when organizations require a large amount of compute power for a limited amount of time).
- When organizations **move on-premises workloads to the cloud by performing a "lift-and-shift"** of existing physical or virtualized servers

IaaS may **NOT** be the best option when:

- Regulatory compliance does not allow workloads to be run off-premises.
- Applications have strict quality-of-service (QoS) requirements.
- Organizations have existing in-house customized infrastructure to meet their IT needs.

# **The Platform-as-a-Service Model**

### **Platform-as-a-Service**

***(definition)* Platform-as-a-service (PaaS) is a model that provides a software platform such as a database server or web server without the complexity of purchasing, installing, and maintaining the underlying hardware and software**

PaaS shifts more of the burden of installation and maintenance from the user to the cloud provider. Azure SQL Database, which is essentially a PaaS version of Microsoft SQL Server in the cloud, provides a handy reference point for contrasting IaaS and PaaS. To run SQL Server in an IaaS scenario, you would deploy a virtual machine, and then connect to it remotely and install SQL Server. Of course, you would be responsible for acquiring a SQL Server license and keeping SQL Server patched and up to date. With Azure SQL Database, you simply create an instance of the service; licensing and maintenance is built in.

PaaS offerings vary among providers but usually feature some basic functionality, which includes:

- **A web-based user interface for using and configuring the platform.**
- **Multitenant architecture in which multiple concurrent users utilize the same tools.**
- **Built-in mechanisms enabling the platform to scale dynamically to meet demand.**

## **Examples of PaaS**

**Google App Engine is an example of a PaaS.** Using Google's APIs, developers can create Web and mobile applications that run on Google's infrastructure. **Azure App Service** is another example, as is AWS Elastic Beanstalk, which provides a managed platform for hosting applications developed with Go, Java, .NET, Node.js, PHP, Python, and Ruby. Cloud providers frequently offer popular databases such as MySQL, Oracle, and SQL Server as PaaS services, too.

One of the more recent innovations in PaaS is that of ***serverless computing***, in which the cloud provider provides a fully managed platform for executing your code. Azure Functions, for example, let you upload code written in C#, JavaScript, Java, or Python to Azure and execute it in response to predefined triggers. Amazon offers a similar service in the form of AWS Lambda, as does Google in Google Cloud Functions.

Yet another example of PaaS that is rapidly gaining in popularity is one that runs Docker containers in the cloud. The platform in this case is a **prebuilt Docker stack**. Running containerized applications in this manner prevents you from having to deploy servers and install and maintain the Docker runtime. Modern cloud platforms support Docker-compatible container registries of their own, which enables containers to start faster by co-locating container images and the containers that run them in the same data center.

## **Pricing Models**

**Unlike the IaaS pricing model, which is typically billed on an hourly basis, PaaS usually is priced based on usage.** To simplify billing and make costs more predictable, cloud providers frequently offer *service tiers* that include specific features and amounts of resource utilization for a specified hourly or monthly price.

As PaaS evolves, pricing models evolve, too. For example, Azure Functions, Amazon Lambda, and Google Cloud Functions offer a consumption-based pricing model in which you only pay for the time a **function executes, and not for time spent waiting for the function to be triggered again**. This is extremely beneficial from a cost perspective for operations that occur relatively infrequently and execute for relatively short periods of time, such as nightly backups or weekly billing runs.

## **Use Cases for PaaS**

PaaS is a compelling model for certain types of applications, such as:

- **Rapid application development scenarios.**
- **Applications that require Web-based infrastructure to handle varying loads from users.**
- **Applications that may not need redeployment or migration to other platforms in the future.**

There are certain scenarios in which PaaS may **NOT** be ideal, such as:

- When the application needs to be highly portable in terms of where it is hosted because PaaS APIs can vary from one PaaS provider to another.
- When proprietary languages or APIs could impact the development process or cause trouble in the future due to vendor lock-in.
- When application performance requires customization of the underlying hardware and software.

# **The Software-as-a-Service Model**

### **Software-as-a-Service**

*(definition)* Software-as-a-service (SaaS) is a software delivery model in which software is hosted in the cloud. The infrastructure, the platform, and the software itself are managed by the cloud provider, and the **software is consumed as a service.**

SaaS is the model in which the cloud provider delivers software as an Internet service, requiring no software installs or maintenance on the part of the user. In cases in which SaaS applications are end-user applications, users access the software through their browsers. The browser loads the latest version of the SaaS application dynamically and transparently.

SaaS has become a common software delivery model for many business applications, including accounting, collaboration, customer relationship management (CRM), management information systems (MIS), enterprise resource planning (ERP), invoicing, human resource management (HRM), content management (CM), as well as service desk management.

With SaaS, the provider maintains the software and infrastructure to run it. The provider routinely develops the software, and e**nhancements are automatically made available to all users each time they log in to the service.** In addition, any **application data that results from the use of the service resides in the cloud and is available to the user from any location.**

A vast majority of SaaS solutions are based on what is referred to as multitenant architecture. In this architecture, a single version of the application, with a single configuration, is used for every customer (referred to as a *tenant*). To enable the service to scale well, it might be installed on several servers on the provider's side. Dynamic scaling is utilized to allow more users to use the service as it becomes more popular.

Typical characteristics of SaaS include:

- **Web-based access to the software service.**
- **Software is managed from a central location by the cloud provider.**
- **Software is delivered in a one-to-many model in which "one" is the cloud provider and "many" are the cloud users.**
- **The cloud provider handles software upgrades and patches.**

## **Examples of SaaS**

**Web mail is one of the early examples of SaaS**. Web mail enables users with a browser and an Internet connection to access their e-mail anywhere at any time. Offerings from Hotmail, Yahoo!, and Gmail are extremely popular. These services are based on the "freemium" model, wherein basic services are free, and more advanced features are available with a subscription. Furthermore, providers earn revenue mainly from advertisements that are displayed to the users as they use the service.

Another popular example of **SaaS is online office suites such as Google Drive and Microsoft 365,** which allow users to create, edit, and share documents online. Google utilizes the freemium model for individual users. Microsoft has a charge model based on the features required and the number of users per month.

SaaS services aren't always built to serve end users directly by offering Web UIs. For example, Azure Stream Analytics and its AWS analog, Amazon Kinesis Analytics, ingest data streaming from apps or IoT devices and permit developers to extract information from the data streams using SQL queries. They are frequently used to compose larger, more sophisticated end-to-end solutions by joining them to other SaaS services.

Increasingly, cloud providers are offering APIs -- functions that are called over the Internet using the HTTP(S) protocol and that serve as building blocks for applications -- as SaaS services. One example is Azure's Computer Vision API, which lets developers write code that identifies objects in photos, generates captions and search metadata for photos, finds faces in photos, and more using artificial intelligence (AI). Amazon offers a similar API called Rekognition, while Google calls its offering the Vision API. **Intelligence APIs offered in the form of SaaS services enable software developers to infuse AI into their apps without becoming AI experts themselves and exemplify the continuing evolution of cloud computing.**

## **Pricing Models**

Unlike traditional software, which is sold under the software licensing model (with an up-front license cost and an optional ongoing support fee), S**aaS providers generally price applications using a monthly or annual subscription fee.** This model enables SaaS to fulfill one of the main purported advantages of cloud computing: reducing the capital expenditure or the up-front cost of software. SaaS providers typically charge based on usage parameters, such as the number of users using the application.

In the case of APIs delivered using the SaaS model, billing is usually performed on a per-call basis. Cloud providers frequently offer a certain number of calls or transactions per month for a flat rate and charge a slightly discounted rate for activity exceeding that threshold.

## **Use Cases for SaaS**

SaaS is the right model for certain types of applications, such as:

- Applications that are fairly standardized and do not require custom solutions. E-mail is a good example of a fairly standardized application.
- Applications that have a significant need for remote/web/mobile access, such as mobile sales management software.
- Applications that have a short-term need, such as collaborative software for a particular project.
- Applications in which demand spikes significantly, such as tax or billing software that is used once a month.

However, there are situations where SaaS may not be the right choice, such as:

- Applications that require offline access to data.
- Applications that require significant customization.
- Applications for which policies or regulations prevent data from being hosted externally.
- Applications for which existing in-house solutions satisfy all of the organization's needs.

# **Types of clouds**

There are three well-known types of clouds: public clouds, private clouds, and hybrid clouds. 

A public cloud is owned by a cloud provider such as Microsoft but is made available to the public. 

A private cloud is typically owned by an organization, which also controls the access to the cloud. 

A hybrid cloud is a combination of public and private clouds. 

# **Public Cloud**

In a public cloud, the cloud infrastructure is owned by a cloud provider and is accessible to the public over the Internet (Figure 1.7). **The cloud provider hosts the cloud infrastructure, and end users can access it remotely without the need to purchase and set up hardware and software.** Public cloud resources are shared among end users. Public-cloud users are typically charged for the duration for which these services are used. However, public-cloud charge models vary across providers and by individual cloud services. The security and terms of use are defined by the provider. Consequently, end users must work within the constraints of the provider when consuming their services.

![https://docs.microsoft.com/en-us/learn/cmu-cloud-admin/cmu-cloud-admin-overview/media/public_cloud.png](https://docs.microsoft.com/en-us/learn/cmu-cloud-admin/cmu-cloud-admin-overview/media/public_cloud.png)

*Figure 1.7: Public cloud.*

A variation on public clouds is ***sovereign clouds* -- public clouds that are physically isolated from other public clouds and to which access is restricted to specific organizations (particularly governments) or countries that place strict requirements on how and where data is stored.** Amazon, Microsoft, and Google run sovereign clouds for the U.S. government. They also offer sovereign clouds in countries such as Germany and China.

# **Private Cloud**

In a private cloud, the cloud infrastructure is owned by an organization (Figure 1.8). The **infrastructure is accessible to specific users via the organization's intranet**. The cloud environment is procured, set up, operated, and maintained by the organization itself. The private cloud resources are typically **shared among an organization's end users**. Unlike the public cloud, security and terms of use of a private cloud are defined by the organization. Because the entire infrastructure is located in the organization, security can be compliant with the organization's policies.

![https://docs.microsoft.com/en-us/learn/cmu-cloud-admin/cmu-cloud-admin-overview/media/private_cloud.png](https://docs.microsoft.com/en-us/learn/cmu-cloud-admin/cmu-cloud-admin-overview/media/private_cloud.png)

*Figure 1.8: Private cloud.*

# **Hybrid Cloud**

In a hybrid cloud, the infrastructure includes an owned private cloud and a leased public cloud (Figure 1.9). Hybrid clouds enable the idea of "cloud bursting," in which an organization uses its private cloud for most of its needs and dynamically provisions resources in the public cloud when utilization exceeds the capacity of its private cloud. If regulatory requirements prevent certain data from being stored off-premises, hybrid clouds provide a solution to that, too.

![https://docs.microsoft.com/en-us/learn/cmu-cloud-admin/cmu-cloud-admin-overview/media/hybrid_cloud.png](https://docs.microsoft.com/en-us/learn/cmu-cloud-admin/cmu-cloud-admin-overview/media/hybrid_cloud.png)

*Figure 1.9: Hybrid cloud.*

Other types of clouds continue to emerge. One example is *community clouds*, which share infrastructure among different organizations that have common security or other concerns. Various non-profit organizations that work closely with government might build and share a community cloud. Another type is *distributed clouds*, which provide cloud users with access to machines at different geographical locations. An example is Cloud@Home, which leverages volunteered resources as a shared resource.

# **Microsoft Azure**

As of this writing, Azure is available in 140 countries and has data centers in more than 50 locations around the world (Figure 1.11). Subsets of Azure are available through Azure Stack, which allows an organization to build a private cloud that can seamlessly connect to and interact with the Azure public cloud. These allow internal data centers to be highly automated, using shared resources that can respond to sudden spurts in demand.

![https://docs.microsoft.com/en-us/learn/cmu-cloud-admin/cmu-cloud-admin-overview/media/azure-regions.png](https://docs.microsoft.com/en-us/learn/cmu-cloud-admin/cmu-cloud-admin-overview/media/azure-regions.png)

*Figure 1.11: Microsoft Azure regions[3](https://azure.microsoft.com/global-infrastructure/regions/).*

Azure features more than 100 IaaS, PaaS, and SaaS services, including:

**Compute**: Microsoft offers Azure Virtual Machines, which can be configured to run Windows or various flavors of Linux. Azure also supports Virtual Machine Scale Sets, which support auto-scaling by creating and deleting VMs automatically in response to changing demand or on a predefined schedule.

**Storage**: The family of services known as Azure Storage offers several storage solutions, including:

- Azure Blobs for storing binary large objects
- Azure Tables for storing NoSQL data
- Azure Queues for connecting services and microservices with persistent storage queues
- Azure Files, which offer SMB-based storage endpoints (Windows-compatible file servers) to mount and store files in the cloud

Azure also offers managed relational database services through **Azure SQL Database**; a managed, multi-model NoSQL database service known as **Cosmos DB;** and high-performance key-value caching through Azure Redis Cache. Microsoft also offers a unique storage appliance called StorSimple, which is a local SSD/HDD storage array that integrates with Azure to provide a hybrid storage solution, and that connects to Azure for backup, analytics, and cloud deployment.

**Networking**: Microsoft also offers virtual private networking services through Azure Virtual Network. Another unique feature of Microsoft's Azure cloud is the ability to purchase dedicated fiber connectivity to Microsoft's data centers through ExpressRoute. Azure Traffic Manager can be used to route traffic to Azure Virtual Machines based on location, performance, and other criteria, as well as to perform DNS-based load balancing. A recent addition to the Azure family of services, Azure Front Door, expands on Traffic Manager's capabilities to let you define, manage, and monitor global routing for web traffic by optimizing for best performance and instant global failover.

**PaaS products**: Azure offers several PaaS products, including **Azure App Service, Azure SQL Database, and Azure Cosmos DB**. In the analytics space, Azure offers several products including HDInsight, which is a managed Hadoop cluster service similar to Amazon's EMR; Azure Databricks, which is a managed Spark implementation; and Data Lake Analytics, which offers per-job pricing for analyzing and transforming massive quantities of data.

**SaaS products**: Microsoft offers a range of SaaS services, two of which are **Microsoft 365 and OneDrive**. Other SaaS services include Azure Cognitive Services, which is a set of services and APIs for incorporating artificial intelligence into apps; **Azure Machine Learning Service**s for building, deploying, and versioning machine-learning models; and Azure Machine Learning Studio, which provides a browser-based, drag-and-drop interface for building, training, and testing machine-learning models.

# **Cloud use cases**

An organization's IT costs are many-fold, which include expenses for hardware and software as well as expenses for support and management. Typically, these costs fall into two categories:

- **Capital expenses (CapEx):** The initial investment for a particular IT service or solution. For example, when an organization decides to implement a software solution to address a particular need -- for example, enterprise resource planning (ERP) -- CapEx would include all physical hardware and software purchases. CapEx investments are for the lifetime of the solution. CapEx are an up-front expense, which are either paid as a lump sum or financed with extra charges.
- **Operational expenses (OpEx):** The recurring costs incurred while operating a particular system. For the ERP case, that would include utility fees (such as power and cooling) to keep the infrastructure running, space leases if the facility is rented, personnel costs to support the system, and software support and license fees. OpEx are typically a monthly recurring charge.

The business model for IT software has evolved over the years into the following forms:

1. **Traditional model**: An organization purchases licensed software, which it then owns and maintains.
2. **Open-source model**: Software is essentially free, but the organization pays vendor support costs.
3. **Outsourcing model**: An organization hires another company, possibly overseas, to manage and maintain the software.
4. **Hybrid model**: A software vendor sells highly standardized software to many clients, along with software management and support, thereby amortizing costs of expertise, software management, and support across several clients.
5. **Cloud computing model**: Software is developed and delivered over the Internet to many clients at lower costs.

## **Reducing Capital Expenditure**

Organizations choose to reduce their capital expenditures so they can limit the commitment of large investments for long-lived IT resources. Shifting expenses away from capital expenditures and into operational expenditures enables organizations to stretch their IT budgets and limit up-front costs. Specifically, organizations opt to make investments that have a bigger return on investment in the short term rather than investing in long-lived IT resources whose value depreciates over time. Operating expenses are pay-as-you-go, meaning organizations pay by the month and get value every month. With cloud computing, they can simply rent the resources and incur little to no capital expenditures.

The cloud-computing paradigm offers a transition of the IT business model from CapEx to OpEx. CapEx in IT systems is a long-term investment that freezes a large sum of money into a single investment. OpEx, on the other hand, is a recurring expense which could enable the company the agility to utilize the funds in other profit-yielding investments.

# **Cloud Service Provider Economics**

For a cloud service provider, long-term CapEx as well as recurring OpEx costs are unavoidable. An important challenge for cloud service providers is to satisfy the demands of their clients while achieving high-average utilization in order to make a profit, which depends on their ability to build data centers with high efficiency and reliability at manageable costs. Cloud service providers amortize their costs over a large number of users.

Cloud service providers build large and reliable data centers in order to attract large volumes of users and maximize their profitability. Just like other utility providers, cloud service providers can then procure and maintain hardware and software at significant savings per unit.

## **Economies of Scale**

Cloud service providers organize their infrastructure into large data centers, which typically leverage three main areas:

- **Supply-side savings**: Large-scale data centers lower the cost per server.
- **Demand-side aggregation**: Aggregating demand for computing allows server utilization rates to increase.
- **Multitenancy efficiency**: When changing to a multitenant application model, increasing the number of tenants (that is, customers or users) lowers the application management and server cost per tenant.

Cloud service providers undertake the difficult task of building and maintaining data centers for users. For this model to be feasible, cloud service providers must leverage economies of scale and bring in many users. Providers benefit from economies of scale in the following areas:

- **Cost of power**: Electricity is rapidly becoming the largest element of total cost of ownership (TCO) in a data center, accounting for approximately 15% to 20% of total costs. Large cloud service providers can place their data centers in locations with lower power costs and sign bulk purchase agreements with electric providers to reduce electricity costs. In some cases, they are building their own power supplies that leverage advancements in solar energy and other green energy sources.
- **Infrastructure labor costs**: Cloud computing enables repetitive management tasks to be automated. In addition, in larger facilities, a single system administrator can service thousands of servers with the use of advanced management software.
- **Buying power**: Cloud service providers can purchase equipment in bulk from manufacturers, which can lead to major discounts over smaller buyers. In addition, cloud providers standardize their servers and equipment, which helps in lowering purchase and support costs compared to smaller IT departments.

Cloud providers such as Amazon, Microsoft, and Google build dozens of data centers all over the world in order to achieve economy of scale and to minimize latency by minimizing the distance between data centers and users.

![Screen Shot 2022-06-12 at 9.13.43 AM.png](Module%201%20951db827e6f9466a9870c44b95a4badf/Screen_Shot_2022-06-12_at_9.13.43_AM.png)

# **Benefits of the Cloud**

The popularity of cloud computing is driven by its numerous benefits, including improved economics, simplified IT management, scalability, flexibility, improved utilization, and a reduced carbon footprint. Let us consider the cloud benefits individually:

- **Economic model**: Organizations typically estimate their IT requirements for a period of 1 to 5 years in advance in a process called *capacity planning*. Capacity planning leads organizations to estimate IT investments for peak loads, which could lead to either excess capacity at times (underutilized resources) or deficient capacity when loads exceed projections (which could lead to service degradation). With the pay-as-you-go economic model, organizations pay for the resources that they need. Organizations no longer have to pay up-front costs, invest in and procure expensive computing infrastructure, or pay recurring costs to manage their infrastructure. This is particularly important for startups because by leasing compute resources, they benefit from reduced up-front cost and reduced time to market when creating and making their offerings available to the general public.
- **Simplified IT management**: Users of cloud services need not allocate time and resources to set up, operate, and maintain their IT resources. The cloud provider, however, competes for clients and hence invests significant resources to manage and maintain their offerings with high reliability.
- **Scalability**: In a traditional, in-house computing environment, organizations can take anywhere from a few days to several months to procure, set up, and operate IT infrastructure. Cloud service providers provision rented computing resources for their clients in a matter of hours or even minutes. Clients not only can scale up resources on-demand, but can scale them down during low-demand periods to save money. Therefore, clouds enable the important property of elasticity, wherein resources can be both provisioned and deprovisioned in a dynamic or programmatic manner to adapt to changing workloads. In order to support elasticity, many cloud service providers make automated scaling solutions available to dynamically alter resource provisioning as demand fluctuates.
- **Flexibility**: For certain cloud services, providers offer their users the flexibility to configure any software platform to run on any available operating system as a virtualized image on custom-provisioned, rented infrastructure. Cloud offers a shift from an organization's inflexible IT design decisions (that are tied to specific development platforms and infrastructure) to more flexible, elastic, and modular choices.
- **Improved utilization**: Resource utilization is significantly improved with cloud computing because physical resources are shared across users (multitenancy). Through virtualization, servers are now consolidated as operating system images that are sharing the same system resources. Hence, utilization is improved, which leads to overall savings in power and cooling and reduces the carbon footprint.
- **Rapid and Global Deployment**: By employing the services of cloud service providers that also have a global data center presence, startups can compete with established players by rapidly rolling out applications and services across a global audience. This is particularly important of social media startups which may see viral growth trends as services become popular across multiple countries.

# **Risks of Cloud Computing**

By embracing cloud services, users and organizations can take advantage of many benefits. However, using these services introduces several risks, such as:

- **Vendor lock-in**: Cloud computing platforms from major cloud vendors are largely proprietary. Lack of standardization can lead to the situation of vendor lock-in, such as when a client signs up for a cloud service, develops applications for it, and deploys data on it. The lack of standardization makes it unlikely that the client can move to another cloud platform seamlessly. The client often requires a third-party cloud migration specialist or an additional service to move the application to a different platform.
- **Security risks**: Because cloud computing with public clouds can result in an organization's data being shipped beyond its four walls, security becomes a primary risk and concern. For certain domains, it is simply unacceptable for users or organizations to do so, in which case they may have to resort to building a private cloud or own resources with restricted access to meet their needs. However, certain markets that have tight security requirements have niche solutions. An example is Amazon GovCloud, which meets certain U.S. federal government requirements for data security and integrity. GovCloud is physically distinct from other cloud infrastructures that Amazon makes available to the public, thereby reducing exposure. Azure Government serves the same role in the Azure space.
- **Privacy risks**: The use of the cloud also raises many privacy-related concerns. Depending on the laws under which a cloud service provider operates, governments may have the power to search and seize data from the provider without the client's explicit consent or notification. Furthermore, clients cannot be fully assured of data confidentiality when using public clouds. We discuss some of security risks associated with cloud computing later in this module.
- **Reliability risks**: Even clouds aren't immune to reliability issues. In December 2012, Netflix users experienced a service outage due to Amazon's "connectivity issues and degraded performance" from their servers in Virginia. Amazon EC2's Northern Virginia data center went down for a few days in 2011, affecting websites such as Reddit and Foursquare. Microsoft Azure also faced a similar problem, and their services went down for 2½ hours in Western Europe. Public clouds pose a potential reliability risk that can affect organizations. Clients must design for failures and use features such as availability zones, in which clients can set up failover and redundant infrastructures to take over in case of failure, which comes at a price, of course. Cloud users attempt to mitigate the cloud reliability risk by signing service-level agreements (SLAs) that enable compensation when exposed to such events. Since cloud services can only be accessed over the network, any disruption of connectivity will cause the application to fail, possibly leading to a loss of reputation and/or revenue.

Some of these risks are not specific only to cloud computing, but are typical for any service provider, be they banking or health services. Cloud service providers must carefully consider the implications of these risks and design solutions to mitigate them as their credibility and reputation directly impacts their rate of adoption. Cloud adopters who offer their own services must also safeguard their reputation against these risks.

# **Challenges in Cloud Computing**

Along with the benefits and risks, there are several challenges associated with the adoption of cloud computing:

- **Application engineering and development**: A cloud inherently offers the promise of on-demand, dynamically scalable infrastructure. Programming a cloud, however, is more complex than writing code for a single machine. New programming paradigms such as MapReduce, Spark, and GraphLab, coupled with provider APIs to manage infrastructure, help developers manage complexity but still present a learning curve. In addition, skilled developers with cloud experience are still relatively rare yet are in high demand. Finally, cloud programming models and APIs are continually evolving, which may add to recurring engineering and development costs.
- **Movement of data**: Use of public clouds typically requires connecting to the cloud over the Internet. Because of this requirement, movement of data to and from the cloud is significantly slower than in an organization's local area network (LAN). Although the cloud allows applications to target large amounts of data (big data), data movement can become a limiting factor for cloud adoption. For example, Amazon and Microsoft allow clients to upload large datasets for free or ship hard disks so the data on them can be loaded into the cloud.
- **Quality of service (QoS):** As mentioned earlier, cloud infrastructure is typically shared among many users. This sharing presents a challenge for cloud providers to offer QoS assurances to their clients. This challenge could disallow certain performance-sensitive applications from being migrated to the cloud. QoS in clouds is an important area in cloud research. For example, regulating I/O bandwidth to specific virtual machines could offer predictable performance for critical applications.

![Screen Shot 2022-06-12 at 9.14.25 AM.png](Module%201%20951db827e6f9466a9870c44b95a4badf/Screen_Shot_2022-06-12_at_9.14.25_AM.png)

# **Service-level agreements**

In this course so far, we have talked about the fundamental ideas behind cloud computing and some of the service models that have emerged under the cloud-computing paradigm. Assuming an organization wants to move their infrastructure and services to a cloud provider, several questions arise. For example, how does an organization:

- Define its requirements in terms of the services that they require from the cloud service provider?
- Identify the type and quantity of the services that it requires?
- Determine the level of service and support that it expects from a cloud provider?
- Monitor and validate the type and quality of service that was guaranteed by the cloud service provider?

When an organization needs to formally state its service requirements in business and legal terms, it defines these requirements in terms of **service-level objectives**, which are defined as follows:

### **Service-Level Objective**

*(definition)* A service-level objective is defined as a key element that defines some aspect of the service which is expected from the service provider

A common service-level objective with cloud service providers, for example, is an up-time guarantee, wherein a service is guaranteed to be available and running within normal operational parameters a specified percentage of the time.

Service-level objectives are typically agreed upon between the client and a service provider in a larger contract known as a **service-level agreement**, or SLA, which is defined as follows:

### **Service-Level Agreement**

*(definition)* A service-level agreement (SLA) is a contract between a service provider (either internal or external) and the client that defines the level of service expected from the service provider

Service-level agreements exist in many industries where a supplier-customer relationship exists for a service that is provided by the supplier to the customer periodically. SLAs in information technology, in their current form, have been used since late 1980s by fixed-line telecom operators as part of their contracts with corporate customers.

A typical SLA may consist of the following components:

- A definition of the services to be provided by the service provider to the client
- Methods to measure performance
- Protocols to manage problems
- A list of customer duties
- Warranties that need to be honored by the service provider
- Procedures involved for disaster recovery
- Process and policies regarding the termination of the agreement

The principles that have governed SLAs for decades are especially pertinent in the cloud-computing industry. It is important that organizations understand what a cloud service provider guarantees, as well as what they do not.

# **SLAs in Cloud Computing**

SLAs have evolved over the years to cater to different types of IT services. The evolution of shared infrastructure services such as clouds have necessitated the use of strong service-level agreements. SLAs by definition can define any level of service, but a well-structured SLA will ideally[1](https://www.educause.edu/ero/article/if-its-cloud-get-it-paper-cloud-computing-contract-issues):

- Codify the specific parameters and minimum levels required for each element of the service, as well as remedies for failure to meet those requirements.
- Affirm the client's ownership of its data stored in the service provider's system and specify the client's rights to get it back.
- Detail the system infrastructure and security standards to be maintained by the service provider, along with the client's rights to audit their compliance.
- Specify the client's rights and cost to continue and discontinue using the cloud service provider's service.

For cloud users, the most important element of an SLA is typically the guaranteed up-time, which varies by service and by provider. Up-time is usually measured in "nines," where three 9s, for example, means 99.9%, four 9s means 99.99%, and so on. Providers frequently offer service credits when SLAs are not met. Amazon, for example, provides customers with a 10% service credit if the monthly up-time for an Elastic Beanstalk instance falls below 99.99%, and a 30% credit if it falls below 99%. A figure such as 99% sounds high, but it means that a service could be unavailable for about 3.5 days per year. That's a long time for a company like Amazon or Expedia whose primary interface to its customers (and means of generating revenue) is via the web.

Up-time guarantees can also vary by configuration and by service tier. Microsoft, for example, guarantees that you will have connectivity to an Azure virtual machine at least 99.99% of the time, but only if two or more instances of the virtual machine are deployed across two or more availability zones in the same Azure region. In addition, some cloud services allow you to select from several service tiers, with higher tiers offering higher guaranteed up-times. In general, the higher the guaranteed up-time, the higher the cost.

# **Auditing in Cloud Computing**

Although cloud computing provides numerous advantages, one of the main challenges is ensuring and verifying the reliability of cloud services. If a customer signs a service contract guaranteeing a certain level of availability, how does the customer know whether the provider is living up to the terms of the contract? For that matter, how does the cloud provider know?

Major cloud providers including Amazon, Microsoft, and Google hire third-party auditors to monitor their platforms for availability and other factors, including data security and confidentiality. The auditors produce *SOC reports* that comply with the American Institute of Certified Public Accountants (AICPA) Service Organization Controls (SOC) standard. SOC reports fall into three categories:

- SOC 1 reports, which cover financial reporting
- SOC 2 reports, which cover security, availability, and privacy
- SOC 3 reports, which also cover security, availability, and privacy

SOC 1 and SOC 2 reports are generally private and are only made available to customers who have signed non-disclosure agreements (NDAs) with the cloud provider. SOC 3 reports are available to the public.

Major cloud providers also offer monitoring services to their clients. These services can be deployed along with IaaS, PaaS, and SaaS services to alert clients in near real-time if, for example, a web site goes down or a VM becomes unavailable. While the responsibility for meeting the terms of the SLA rests largely with the cloud provider, customers, too, can architect the solutions that they deploy to maximize availability -- for example, by employing failover mechanisms offered by the cloud provider to make sure that traffic to a database or VM that has become unavailable is redirected to a copy of that database or VM in another region.

Given the nature of cloud services, auditing and monitoring is necessary. This requires real-time monitoring and evaluation in order to trigger a rapid response to safeguard the client's service and reputation. In public clouds, this must be achieved while preventing the exposure of client data to other cloud clients. Near real-time auditing is rapidly evolving and becoming a requirement for reliable cloud computing services which will require audit trails and monitoring of service, performance, and security metrics among others.

![Screen Shot 2022-06-12 at 9.15.36 AM.png](Module%201%20951db827e6f9466a9870c44b95a4badf/Screen_Shot_2022-06-12_at_9.15.36_AM.png)

# **Cloud use cases**

With the rapid evolution of cloud technologies, new use cases emerge every day. In this section, we discuss some of the common and most compelling use cases for the cloud and present some real-world examples.

# **Web and Mobile Applications**

A main driver for cloud computing comes from web hosting. Web sites and web applications typically are hosted on a server with a dedicated internet connection. Older web hosting services either provided dedicated servers to clients or gave a fraction of a larger UNIX system to multiple clients. Now, with the advent of cloud computing, web and mobile applications can be built on top of existing IaaS, PaaS, and even SaaS services.

- **SaaS-based**: Using the SaaS model, organizations can deploy one-size-fits-all applications on the web. Common examples include web mail, social networking sites, and utility web sites such as personal organizers, calendars, and planners.
- **PaaS-based**: Application developers can use a range of online platforms and tools to create PaaS and mobile applications. Platforms such as Google App Engine, Parse, and AppScale are popular platforms on which web and mobile applications can be built.
- **IaaS-based**: Applications that need even more customization and flexibility can adopt the IaaS model by renting out virtual machines from providers such as EC2 and Rackspace and deploy a fully customized software stack to run the Web application.

Consider the following scenarios:

- Animoto, an online video slideshow creator, decided to deploy a Facebook application. Traffic to the service surged, which resulted in Animoto scaling up from 50 servers to 3,500 servers in 3 days. Such elastic scalability is made possible through cloud computing.
- Online retail stores that use cloud computing, such as Amazon and Target.com, have been able to size up infrastructure for peak activity (such as the day after Thanksgiving). Salesforce.com hosts customers ranging from those with 2 seats to more than 20,000 seats, all using the same Web platform.

Domino's Pizza relies on a combination of services provided by AWS and Azure to power its business. For example, it uses Azure App Service and Azure Cosmos DB -- both PaaS services -- for online ordering and delivery tracking, enabling it to scale up and down as needed to meet demand. Domino's also uses SaaS-based Microsoft Dynamics ERP to meet its enterprise resource planning needs[1](https://news.microsoft.com/en-au/features/azures-platform-service-pepperoni-pizza-dominos/).

# **Big-Data Analytics**

Many organizations have to deal with large amounts of data. This data may emanate from such areas as sensors, experiments, transactional data, and web-page activity. Big-data processing usually requires a lot of computational and storage resources but, depending on an organization's needs, may be periodic or seasonal. For example, Amazon may have business intelligence and analytics jobs set up for the end of the day, which may require a few hours of time from a few hundred servers. In these scenarios, cloud computing makes sense because these resources can be acquired on demand. Many firms even have fully automated analytics pipelines that automatically collect, analyze, and store data, with resources being provisioned on demand. Examples of big data scenarios include the following:

- The Union Pacific Railroad mounts infrared thermometers, microphones, and ultrasound scanners alongside its tracks. These sensors scan every train as it passes and send readings to the railroad's data centers, in which pattern-matching software identifies equipment at risk of failure.
- Traditional retailers such as Walmart are following in the footsteps of online retailers such as Amazon by analyzing consumer spending habits to offer personalized marketing campaigns and offers to individual customers.
- Companies such as Time Warner and Comcast are using big data to track media consumption habits of their subscribers and provide value-added information to advertisers and customers. The video games industry tracks the gameplay habits of millions of console owners. Companies such as Riot Games sift through 500GB of structured data and over 4TB of operational logs every day.

The cloud also plays a role in analytics by offering managed implementations of popular tools such as Apache Hadoop and Apache Spark that can be ready to use in minutes, and subsequently deleted when no longer needed. Purchasing, deploying, and configuring a Spark cluster can take months and requires significant expertise. With the cloud, and with services such as Azure Databricks and Databricks in AWS, clusters provisioned with the latest version of Spark can be deployed with a few button clicks.

# **On-Demand High-Performance Computing**

Modern science is impossible without high-performance computing (HPC). In addition to physical experimentation, computer-based simulation has become popular in fields ranging from astrophysics and quantum mechanics to oceanography and biochemistry. Such workloads are computationally demanding and typically are run on dedicated clusters or at supercomputing facilities.

Scientists are increasingly looking to the cloud to fulfill the need for HPC resources. For HPC use, AWS, Azure, and GCP offer extremely powerful virtual machines with faster CPUs and state-of-the-art GPUs. Scientists find the availability of vast amounts of computational power appealing, particularly for small projects or time-sensitive, bursty analytics such as experimental runs before submitting deadlines for research papers. Examples of HPC in the cloud include the following:

- A 3,809-instance EC2 cluster was set up by Cycle Computing, Inc. for a pharmaceutical company to run molecular modeling jobs. The cluster has a total of 30,472 cores, 26.7TB of RAM, and 2PB of disk storage.
- Companies such as Pfizer, Unilever, Spiral Genetics, Integrated Proteomics Applications, and Bioproximity run bioinformatics and genomics workloads on Amazon EC2 instances.
- NASA JPL uses high-performance Amazon EC2 instances to process high-resolution satellite images.

In 2015, risk-management and financial-services firm Willis Tower Watson teamed with Microsoft to deploy an HPC cluster containing 100,000 cores and used it to calculate the cost of providing insurance to every person in the world[2](https://azure.microsoft.com/blog/what-would-you-do-with-100000-cores-big-compute-at-global-scale/). The entire process, from provisioning to downloading the results, took less than 12 hours. On a single-core CPU, the same job would have required almost 20 years to execute.

# **Online Storage and Archival**

One of the important resources that is available through cloud computing is storage. From personal storage solutions, such as Dropbox, to large-scale Internet storage systems, such as Amazon S3, online storage is a major cloud computing use case. The online storage options include:

- **Web-based object storage**: Services such as Amazon S3 and Azure Storage allow users to store terabytes of data as simple objects that can be accessed over HTTP. Many websites use Amazon S3 and Azure Storage to store static content, such as images.
- **Backup and recovery**: Services such as CrashPlan and Carbonite provide online backup of customer data, which is a great option as a secure, off-site backup solution.
- **Media streaming and content distribution**: Services such as Amazon CloudFront and Azure CDN not only store large amounts of data, but assist in content delivery. Requests to pull data from CloudFront are automatically routed to the nearest server, thereby decreasing latency for time-sensitive media, such as video.
- **Personal storage**: Services such as Dropbox and Google Drive are popular among users to store personal documents online for anytime, anywhere access.

# **Rapid Application Development and Testing**

One of the major advantages of the cloud is the ability to rapidly deploy and test applications. An entire computing environment can be deployed in minutes and then torn down and discarded once testing is complete. For many companies, the value is in allowing developers to quickly create enhancements and features and test them without any risk. Specialized hardware and servers do not need to be ordered and installed. Within minutes, a virtual server can be spun up in Amazon EC2, Azure Virtual Machines, or Google Compute Engine. Applications can also be easily stress/load tested. Existing servers can be cloned to perform scalability studies as well.

The cloud is also fueling investments in *DevOps*, which increases an organization's ability to deliver software in less time while achieving higher quality. Organizations create DevOps pipelines that include virtual build servers coupled to cloud-based source-code repositories. Developers check code changes into the repository, which triggers an automatic build in a process known as *continuous integration* (CI). Another feature of DevOps is *continuous delivery* (CD), in which updated builds are automatically tested and rolled out to a staging or production environment. Products such as AWS CodePipeline, AWS CodeBuild, and Azure DevOps enable these types of scenarios and are rapidly becoming the backbone of the software-development process at enterprises large and small.

# **Machine Learning and Artificial Intelligence**

Machine learning (ML) and the subset of machine learning (ML) known as artificial intelligence (AI) are touching lives every day. Credit-card companies use ML/AI models to check for fraud in real time; retailers use them to entice customers with additional purchases and forecast demand; linguists use them to translate speech in real time; restaurants use them to improve food quality3; and researchers use them to analyze the human genome for cancer indicators[4](https://www.sciencedirect.com/science/article/pii/S2001037014000464).

Training ML/AI models frequently requires vast quantities of compute power. One example is the *convolutional neural network* (CNN), which excels at tasks involving computer vision -- for example, determining whether a photo contains a picture of a dog or a cat. These models are typically trained with batches of images comprising billions of pixels and frequently contain 100 or more layers. Training a CNN of this size is impractical on a single computer, but is accomplished relatively efficiently on an HPC cluster equipped with GPUs to facilitate parallel processing.

A recent trend in cloud computing has providers making ML and AI available as SaaS services. With Azure's Custom Vision Service, for example, you can train a neural network to discriminate between cat pictures and dog pictures in a matter of minutes and at virtually no cost. Specific examples of companies that use cloud-based ML and AI services to improve their business processes include:

- Expedia uses machine-learning services powered by AWS's recommendations engine to streamline the hotel booking process[5](https://www.phocuswire.com/Expedia-Partner-Solutions-machine-learning).
- British Petroleum (BP) uses Azure Machine Learning to improve its ability to forecast recovery factors -- the percentage of hydrocarbons that can be extracted from an underground deposit -- for potential oil and gas reservoirs[6](https://customers.microsoft.com/story/bp-mining-oil-gas-azure-machine-learning).
- iGenius combined Google's Cloud AI machine-learning APIs with Google App Engine to build the world's first digital marketing advisor[7](https://cloud.google.com/customers/igenius/).

The explosion in ML and AI research, investment, advances, and applications in recent years is primarily attributable to cloud computing, which makes the vast amounts of computing power required to train new models and improve existing ones available to researchers at institutions large and small.

![Screen Shot 2022-06-12 at 9.16.12 AM.png](Module%201%20951db827e6f9466a9870c44b95a4badf/Screen_Shot_2022-06-12_at_9.16.12_AM.png)

# **Summary**

Here is a brief summary of the important concepts introduced in this module:

- Cloud computing is the delivery of computing as a service over a network, whereby distributed resources are provided to the end user as a utility.
- The idea of utility computing originated in the 1950s and 1960s, but the enabling technologies evolved decades later and have finally matured to a state in which cloud computing is a viable option for organizations to invest in.
- The enabling technologies of cloud computing are networks, virtualization and resource management, utility computing, programming models, parallel distributed computing, and storage technologies.
- The three primary cloud service models are Infrastructure-as-a-Service (IaaS), Platform-as-a-Service (PaaS), and Software-as-a-Service (SaaS).
- In the IaaS model, providers rent out compute resources in the form of virtual-machine instances, which are available in a variety of CPU, memory, disk, and network bandwidth configurations.
- PaaS provides a platform for running applications that you install and maintain, but prevents you from having to install and maintain the underlying hardware and software.
- SaaS enables the delivery of software over the Internet (typically through a Web browser).
- There are three well-known types of clouds: public, private, and hybrid.
- Popular cloud providers include Amazon Web Services, Microsoft Azure, and Google Cloud Platform. Each provider typically offers a stack consisting of compute, storage, and networking services as well as PaaS and SaaS services.
- Some of the most popular use cases for the cloud include: web and mobile applications, big data analytics, on-demand high performance computing, online storage and archival, rapid application development and testing, and machine learning and artificial intelligence.
- Cloud computing offers many benefits, including a pay-as-you-go economic model, simplified IT management for users, scalability, flexibility, improved utilization, and a decreased carbon footprint.
- Cloud computing also has many risks and challenges, including vendor lock-in, security risks, privacy-related concerns, developer training and reengineering, evolving tools, and movement of data.
- Cloud computing offers a compelling economic model for businesses through the pay-as-you-go model and can significantly lower management and overall costs of IT.
- Cloud service providers leverage economies of scale to provide services at low costs. They require large data centers and many clients in order to amortize the costs over the entire user base.
- Service-level objectives (SLOs) allow an organization to formally state its service requirements to a service provider
- A service-level agreement (SLA) is a contract between a service provider and a client that defines the level of service expected from the service provider.
- Auditing evaluates whether the cloud services comply with the SLA. Cloud service providers hire external auditors to evaluate their services, and the auditors issue SOC reports summarizing compliance and performance.